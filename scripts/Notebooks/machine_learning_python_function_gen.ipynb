{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../machine_learning.py\n",
    "\n",
    "\"\"\"\n",
    "Script aims to read in the reference_dataframe file, select a taxonomic\n",
    "level and group, and read the path to the location of that data. It then\n",
    "prepares data for machine learning by converting base pair coding to numerical\n",
    "encoding, pads it out and then runs the algorithm\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Dropout, MaxPooling1D, Flatten\n",
    "from keras.utils import plot_model\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "def max_seq_len(SeqIO_dict):\n",
    "    \"\"\"\n",
    "    Function takes a SeqIO_dict and returns the lengths of the\n",
    "    longest sequence\n",
    "    \"\"\"\n",
    "    total_lens = []\n",
    "    for key in SeqIO_dict.keys():\n",
    "        total_lens.append(len(SeqIO_dict[key].seq))\n",
    "    return max(total_lens)\n",
    "\n",
    "\n",
    "def numberfy(SeqIO_dict, seq_len, nsubsample, genus_name, species_name):\n",
    "    \"\"\"\n",
    "    Take SeqIO_dict and return SeqIO_dict were bases have been replaced\n",
    "    with numbers\n",
    "    ACGT- replaced with 01234\n",
    "    Take the seq_len each sequence should have\n",
    "    \"\"\"\n",
    "    num_dict = {}\n",
    "    \n",
    "    keys = list(SeqIO_dict.keys())\n",
    "    randkeys = random.sample(keys, k=nsubsample)\n",
    "    \n",
    "    with open(data_root+'models/ids_%s_%s_%s_%s_%s.txt' % (args.tax_rank,args.name,args.n_reads,genus_name,species_name), 'w+') as file:\n",
    "        file.writelines(\"%s\\n\" % key for key in randkeys)\n",
    "    \n",
    "    \n",
    "    for key in randkeys:\n",
    "        seq = str(SeqIO_dict[key].seq).replace(\"A\",'0 ')\\\n",
    "        .replace(\"C\",'1 ').replace(\"G\",'2 ').replace(\"T\",'3 ')\\\n",
    "        .replace(\"a\",'0 ').replace(\"c\",'1 ').replace(\"g\",'2 ')\\\n",
    "        .replace(\"t\",'3 ')\n",
    "#         seq_new = seq + '4 '*(seq_len - int(len(seq)/2))\n",
    "        seq_new = seq + '4 '*(5000 - int(len(seq)/2))\n",
    "        if seq_new.find('t') != -1:\n",
    "            print(seq_new.find('t'))\n",
    "            print(\"ERROR - strange value in sequence\")\n",
    "            print(seq_new)\n",
    "            exit()\n",
    "        num_dict[key] = list(map(int, seq_new.split(' ')[:-1]))\n",
    "    return num_dict\n",
    "\n",
    "\n",
    "def get_model(X_train, Y_train, num_class):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(5000,1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    print()\n",
    "    model.fit(np.expand_dims(X_train,2), Y_train, validation_data=(np.expand_dims(X_test,2), Y_test), batch_size=128, epochs=10, verbose=1)\n",
    "    return model\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Script aims to read in the reference_dataframe file, select a taxonomic\n",
    "level and group, and read the path to the location of that data. It then\n",
    "prepares data for machine learning by converting base pair coding to numerical\n",
    "encoding, pads it out and then runs the algorithm\n",
    "\"\"\")\n",
    "parser.add_argument(\"ref_df_fn\", help=\"File path to the reference dataframe\")\n",
    "parser.add_argument(\"data_root\", help=\"Root folder for analysis/\")\n",
    "parser.add_argument(\"--tax_rank\", \"-r\", help=\"taxonomic rank for analysis\")\n",
    "parser.add_argument(\"--name\", \"-n\", help=\"name of rank to select from\")\n",
    "parser.add_argument(\"--n_reads\", \"-c\", help=\"count of reads per class\")\n",
    "parser.add_argument(\"--one\", \"-1\", help=\"first species to test. not used if tax_rank and name present\")\n",
    "parser.add_argument(\"--two\", \"-2\", help=\"second species to test. not used if tax_rank and name present\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# assign required arguments to variables\n",
    "ref_df_fn = args.ref_df_fn\n",
    "data_root = args.data_root\n",
    "\n",
    "# assign a number of reads per class\n",
    "n_reads = int(args.n_reads)\n",
    "\n",
    "# test to make sure both required file paths are input\n",
    "try:\n",
    "    os.path.exists(ref_df_fn)\n",
    "except:\n",
    "    print('Cannot find %s' % ref_df_fn)\n",
    "try:\n",
    "    os.path.exists(data_root)\n",
    "except:\n",
    "    print('Cannot find %s' % data_root)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[1;34m' + \"Reference dataframe is at \" + ref_df_fn + '\\033[0m')\n",
    "    print('\\033[1;34m' + \"Root directory is at \" + data_root + '\\033[0m')\n",
    "    if args.tax_rank and args.name:\n",
    "        print('\\033[1;34m' + \"Tax Rank is \" + args.tax_rank.lower() + '\\033[0m')\n",
    "        print('\\033[1;34m' + \"Name is \" + args.name.lower() + '\\033[0m')\n",
    "    elif args.one and args.two:\n",
    "        print('\\033[1;34m' + \"Species one is \" + args.one.lower() + '\\033[0m')\n",
    "        print('\\033[1;34m' + \"Species two is \" + args.two.lower() + '\\033[0m')\n",
    "    print('\\033[1;34m' + \"Count of reads per sample is\", n_reads,'\\033[0m')\n",
    "    \n",
    "# read in the reference dataframe from the argument path\n",
    "ref_df = pd.read_csv(ref_df_fn, index_col=None)\n",
    "\n",
    "# check whether the reference dataframe implies there are enough reads\n",
    "# to continue given n_reads\n",
    "try:\n",
    "    if ref_df[ref_df[\"# for use\"] \\\n",
    "              < n_reads].shape[0] > 0 :\n",
    "        print(\"These species need more reads.\")\n",
    "        print(ref_df[ref_df[\"# for use\"] \\\n",
    "              < n_reads])\n",
    "        exit()\n",
    "except:\n",
    "    print('Check %s to have the wanted column names' % ref_df_fn)\n",
    "    \n",
    "# assign flagged variables as lower case and assign indices dataframe\n",
    "if args.tax_rank and args.name:\n",
    "    tax_rank = args.tax_rank.lower()\n",
    "    name = args.name.lower()\n",
    "    try:\n",
    "        indices = ref_df[ref_df[tax_rank] == name].index\n",
    "        print(indices)\n",
    "    except:\n",
    "        print(\"Tax_rank or Name not found in reference_dataframe\")\n",
    "        print(tax_rank, name)\n",
    "elif args.one and args.two:\n",
    "    one = args.one.lower()\n",
    "    two = args.two.lower()\n",
    "    try:\n",
    "        indices = ref_df[(ref_df['species'] == one)&(ref_df['species'] == two)].index\n",
    "        print(indices)\n",
    "    except:\n",
    "        print(\"Species inputs not found in reference_dataframe\")\n",
    "        print(one, two)\n",
    "\n",
    "# where the values are that index's path's dataframe\n",
    "SeqIO_dicts = {}\n",
    "for index in indices:\n",
    "    fasta_path = ref_df.loc[index, 'path for use']\n",
    "    try:\n",
    "        SeqIO_dicts[index] = SeqIO.to_dict(SeqIO.parse(fasta_path, \"fasta\"))\n",
    "    except:\n",
    "        print('Check location of fasta files')\n",
    "        print(fasta_path, \"does not exist\")\n",
    "        \n",
    "# each path within an index corresponds to a species\n",
    "# if tax_rank > genus, we want to look at which species are within which genus/family/order etc.\n",
    "\n",
    "# determine the maximum sequence length of accepted sequences\n",
    "total_lens = []\n",
    "for key, value in SeqIO_dicts.items():\n",
    "    total_lens.append(max_seq_len(value))\n",
    "print('\\033[0;32m'+\"The maximum sequence length of all sampled sequences is\"+ '\\033[1;37m',max(total_lens),'\\033[0m')\n",
    "\n",
    "\n",
    "# randomly subsample n_reads number of reads from each index's corresponding\n",
    "# set of reads, convert base pair coding to numerical coding and \n",
    "# pad to the max sequence length\n",
    "numSeqIO_dicts = {}\n",
    "max_len = max(total_lens)\n",
    "del total_lens\n",
    "if (args.one and args.two) or tax_rank == \"genus\":\n",
    "    for key, value in SeqIO_dicts.items():\n",
    "        numSeqIO_dicts[key] = numberfy(value, max_len, n_reads,ref_df.loc[key,'genus'], ref_df.loc[key,'species'])\n",
    "else:\n",
    "    location = (ref_df.columns.get_loc(tax_rank)-1)\n",
    "    col_name = ref_df.columns[location]\n",
    "    if args.verbose:\n",
    "        print('location is', col_name)\n",
    "\n",
    "    classes = ref_df.iloc[indices,location].unique()\n",
    "    if args.verbose:\n",
    "        print('classes are', classes)\n",
    "\n",
    "    count_dict = {}\n",
    "    for class_ in classes:\n",
    "        count_dict[class_] = sum(ref_df.iloc[indices,location] == class_)\n",
    "    if args.verbose:\n",
    "        print('count_dict is', count_dict)\n",
    "    \n",
    "    del classes\n",
    "        \n",
    "    min_vals = []\n",
    "    for class_, n_class in count_dict.items():\n",
    "#         if n_class == min(count_dict.values()):\n",
    "            min_vals.append(ref_df[ref_df.iloc[:,location] == class_]['# for use'].min())\n",
    "    if min(min_vals) % 2 == 0:\n",
    "        minimum_value = int(min(min_vals))\n",
    "    else:\n",
    "        minimum_value = int(min(min_vals)-1)\n",
    "    if minimum_value > 35000:\n",
    "        minimum_value = 35000\n",
    "        \n",
    "    del min_vals\n",
    "    \n",
    "    if args.verbose:\n",
    "        print('minimum number of reads is', minimum_value)\n",
    "    class_lens_ind = []\n",
    "    if len(count_dict) > 1:\n",
    "        max_reads = 0\n",
    "        for key, value in count_dict.items():\n",
    "            if value == max(count_dict.values()):\n",
    "                max_reads = value*n_reads\n",
    "\n",
    "        if max_reads <= minimum_value:\n",
    "            minimum_value = max_reads\n",
    "\n",
    "        for key, n_class in count_dict.items():\n",
    "            s_reads = int(minimum_value/n_class)\n",
    "            if ref_df[ref_df.loc[:,col_name]==key]['# for use'].min() < s_reads:\n",
    "                minimum_value = ref_df[ref_df.loc[:,col_name]==key]['# for use'].min()/n_class\n",
    "                s_reads = int(minimum_value/n_class)\n",
    "            if args.verbose:\n",
    "                print('The class is', key, 'and the number of reads to be subsampled is', s_reads)\n",
    "            for keya, value in SeqIO_dicts.items():\n",
    "                if ref_df.loc[keya,col_name] == key:\n",
    "                    numSeqIO_dicts[keya] = numberfy(value, max_len, s_reads, ref_df.loc[keya,'genus'], ref_df.loc[keya,'species'])\n",
    "                    class_lens_ind.append(s_reads)\n",
    "        n_reads = minimum_value\n",
    "    elif len(count_dict) == 1:\n",
    "        s_reads = n_reads\n",
    "        print(\"no comparison for the rank\")\n",
    "        exit()\n",
    "\n",
    "try:\n",
    "    del count_dict\n",
    "except:\n",
    "    pass\n",
    "\n",
    "location = (ref_df.columns.get_loc(tax_rank)-1)\n",
    "col_name = ref_df.columns[location]\n",
    "classes = ref_df.iloc[indices,location].unique()\n",
    "\n",
    "del indices\n",
    "\n",
    "order = []\n",
    "seq_list = []\n",
    "total_expected_reads = len(classes)*n_reads\n",
    "class_lens = []\n",
    "for class_ in classes:\n",
    "    tmp_sum = []\n",
    "    for key in numSeqIO_dicts.keys():\n",
    "        if ref_df.loc[key,col_name] == class_:\n",
    "            order.append(key)\n",
    "            seq_list.append(np.array(list(numSeqIO_dicts[key].values())))\n",
    "            tmp_sum.append(len(list(numSeqIO_dicts[key].values())))\n",
    "    class_lens.append(sum(tmp_sum))\n",
    "\n",
    "try:\n",
    "    del tmp_sum\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del numSeqIO_dicts\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "total_actual_reads = min(class_lens)\n",
    "\n",
    "print(class_lens)\n",
    "if args.verbose:\n",
    "    print(\"Ids order for labels is\", order)\n",
    "    if not (args.one and args.two) and tax_rank != \"genus\":\n",
    "        print(\"Number of reads subsampled per id is\", class_lens_ind)\n",
    "    print(\"Total expected reads is\", total_expected_reads)\n",
    "    for i in range(0, len(classes)):\n",
    "        print(classes[i], \"has\", class_lens[i], \"reads\")\n",
    "    print(\"Total reads used per class is\", sum(class_lens))\n",
    "    print(\"Total actual reads available per class is\", total_actual_reads)\n",
    "\n",
    "try:\n",
    "    del class_lens_ind\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "seq_comb = np.concatenate(seq_list, axis = 0)\n",
    "num_class = len(classes)\n",
    "\n",
    "try:\n",
    "    del seq_list\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if len(set(class_lens)) == 1:\n",
    "    all_data = seq_comb\n",
    "else:\n",
    "    class_lens_cumsum = np.cumsum(class_lens)\n",
    "    new_seq_list = []\n",
    "    for i in range(0, len(class_lens_cumsum)):\n",
    "        if i == 0:\n",
    "            new_seq_list.append(seq_comb[0:class_lens_cumsum[i]][:total_actual_reads])\n",
    "        else:\n",
    "            new_seq_list.append(seq_comb[class_lens_cumsum[i-1]:class_lens_cumsum[i]][:total_actual_reads])\n",
    "    del seq_comb\n",
    "    all_data = np.concatenate(new_seq_list, axis = 0)\n",
    "\n",
    "try:\n",
    "    del class_lens\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del new_seq_list\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# determine the number of classes and generate an array of ids\n",
    "all_labels_onehot = np.zeros( (total_actual_reads*num_class,num_class) )\n",
    "for i in range(0, num_class):\n",
    "    all_labels_onehot[i*total_actual_reads:(i+1)*total_actual_reads,i] = 1\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "if args.verbose:\n",
    "    print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "    print('all_data.shape:', all_data.shape)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# classes_dict = {}\n",
    "# for i in range(0, len(classes)):\n",
    "#     classes_dict[classes[i]] = all_data[i*total_actual_reads:(i+1)*total_actual_reads,:]\n",
    "\n",
    "# # Print an entry to visualise this\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# for entry in classes_dict:\n",
    "#     print(classes_dict[entry][50])\n",
    "#     if args.verbose:\n",
    "#         print('%s all_data shape:' % entry, classes_dict[entry].shape)\n",
    "\n",
    "samples_count = total_actual_reads*num_class\n",
    "if args.verbose:\n",
    "    print('samples_per_class:', total_actual_reads)\n",
    "    print('samples_count:', samples_count)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "if args.verbose:\n",
    "    print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*all_data.shape[0])\n",
    "if args.verbose:\n",
    "    print(\"Training data size:\", train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "try:\n",
    "    del train_size\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del samples_count\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del shuffle_indices\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = all_data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "X_test = all_data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "try:\n",
    "    del all_data\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if args.verbose:\n",
    "    print('X_train.shape : ', X_train.shape)\n",
    "    print('X_test.shape : ', X_test.shape)\n",
    "    print('Y_train.shape : ', Y_train.shape)\n",
    "    print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# run the model as defined in the get_model function\n",
    "# model = get_model(X_train, Y_train, num_class)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(num_class, activation='softmax'))\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classes_dict = {}\n",
    "for i in range(0, len(classes)):\n",
    "    classes_dict['%s' % i] = classes[i]\n",
    "\n",
    "# model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1],1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "print()\n",
    "history = model.fit(np.expand_dims(X_train,2), Y_train, validation_data=(np.expand_dims(X_test,2), Y_test), batch_size=128, epochs=10, verbose=1)\n",
    "\n",
    "# plot? the history of the model training accuracy vs val_accuracy\n",
    "    # could probably put this into a function as well\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "model.save(data_root+'models/model_%s_%s_%s.h5' % (args.tax_rank,args.name,args.n_reads))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for %s %s' % (name, tax_rank))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(data_root+'plot_histories/history_%s_%s_%s.png' % (args.tax_rank,args.name,args.n_reads))\n",
    "plt.close()\n",
    "\n",
    "yhat_probs = model.predict(np.expand_dims(X_test,2), verbose=0)\n",
    "yhat_classes = model.predict_classes(np.expand_dims(X_test,2), verbose=0)\n",
    "print(yhat_classes.shape)\n",
    "print(yhat_classes)\n",
    "\n",
    "Y_test_ints = np.where(Y_test==1)[1]\n",
    "print(Y_test_ints.shape)\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:]\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test_ints, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test_ints, yhat_classes, average=None)\n",
    "print('precision: ', precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test_ints, yhat_classes, average=None)\n",
    "print('recall: ', recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test_ints, yhat_classes, average=None)\n",
    "print('f1: ', f1)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(Y_test_ints, yhat_classes)\n",
    "print(matrix)\n",
    "crosstab = pd.crosstab(Y_test_ints, yhat_classes, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "print(crosstab)\n",
    "data = [accuracy]\n",
    "datacol =['accuracy']\n",
    "count = 1\n",
    "for i in precision:\n",
    "    data.append(i)\n",
    "    datacol.append('precision%i'%count)\n",
    "    count += 1\n",
    "count = 1\n",
    "for i in recall:\n",
    "    data.append(i)\n",
    "    datacol.append('recall%i'%count)\n",
    "    count += 1\n",
    "count = 1\n",
    "for i in f1:\n",
    "    data.append(i)\n",
    "    datacol.append('f1%i'%count)\n",
    "    count += 1\n",
    "stats = pd.DataFrame(data=[data],columns=datacol)\n",
    "stats.to_csv(data_root+'models/stats_%s_%s_%s.csv' % (args.tax_rank,args.name,args.n_reads))\n",
    "crosstab.to_csv(data_root+'models/confusion_%s_%s_%s.csv' % (args.tax_rank,args.name,args.n_reads))\n",
    "with open(data_root+'models/keys_%s_%s_%s.csv' % (args.tax_rank,args.name,args.n_reads), 'w+') as f:\n",
    "    for key in classes_dict.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,classes_dict[key]))\n",
    "if args.verbose:\n",
    "    print(classes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
